{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4ly85dOjwrP"
   },
   "source": [
    "# Assignment 1: Sentiment Polarity for Movie Reviews\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rY-34D1MwuBd"
   },
   "source": [
    "## Part 1: Parsing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ko1YHRcW41kv",
    "outputId": "349c4e37-0e52-4bda-f316-111a18e5f491",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-21 09:29:22--  http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
      "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
      "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File â€˜review_polarity.tar.gzâ€™ not modified on server. Omitting download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -N http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Open file directories\n",
    "pos_dir = 'txt_sentoken/pos/'\n",
    "neg_dir = 'txt_sentoken/neg/'\n",
    "pos_files = os.listdir(pos_dir)\n",
    "neg_files = os.listdir(neg_dir)\n",
    "\n",
    "\n",
    "X_raw = []\n",
    "y = []\n",
    "# Append positive reviews to X_raw and 1s to y\n",
    "for file in pos_files:\n",
    "    with open(pos_dir+file, 'r') as infile:\n",
    "        text = ''\n",
    "        for line in infile:\n",
    "            line = line.replace('\\n','')\n",
    "            text += line.lower()\n",
    "        X_raw.append(text)\n",
    "        y.append(1)\n",
    "        \n",
    "# Append negative reviews to X_raw and -1s to y        \n",
    "for file in neg_files:\n",
    "    with open(neg_dir+file, 'r') as infile:\n",
    "        text = ''\n",
    "        for line in infile:\n",
    "            line = line.replace('\\n','') \n",
    "            text += line.lower()\n",
    "        X_raw.append(text)\n",
    "        y.append(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_F1ortjjs_u"
   },
   "source": [
    "## Part 2: Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Very rudimentary 'tokenization'\n",
    "split = []\n",
    "for text in X_raw:\n",
    "    words = text.split()\n",
    "    split.extend(words)\n",
    "\n",
    "# Generate vocabulary\n",
    "vocabulary = set()\n",
    "for word in split:\n",
    "    vocabulary.add(word)\n",
    "\n",
    "# Create ordered vocabulary\n",
    "ordered_vocabulary = sorted(vocabulary)\n",
    "\n",
    "# Create matrix of zeros for X\n",
    "X = np.zeros((len(X_raw), len(ordered_vocabulary)))\n",
    "\n",
    "# Replace 0 with 1 if word in the vocabulary appears in the document \n",
    "for i, text in enumerate(X_raw):\n",
    "    for j, word in enumerate(ordered_vocabulary):\n",
    "        if word in text:\n",
    "            X[i, j] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Learning framework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inserting the pseudo input 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting pseudo input\n",
    "X = np.insert(X, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training/test split**\n",
    "\n",
    "Take the middle 400 elements as the test set so there's an even positive/negative split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "X_test = X[800:1200]\n",
    "y_test = y[800:1200]\n",
    "\n",
    "# Training set\n",
    "X_training = np.delete(X, np.s_[800:1200], axis=0)\n",
    "y_training = y\n",
    "y_training[800:1200] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing model class\n",
    "\n",
    "Stopping criteria for fit function when loss reaches 100. I still received 100% accuracy on the training set, and it sped up training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, learning_rate, dampening):\n",
    "        self.gamma = learning_rate\n",
    "        self.lamb = dampening\n",
    "        self.omega = np.full((len(X[0])), -.01)        # Set arbitrary omega to start\n",
    "\n",
    "        \n",
    "        \n",
    "    # Loss function\n",
    "    def loss(self, X, y):    \n",
    "        sigma = 0\n",
    "        for i in range(len(X)):\n",
    "            margin = np.dot(self.omega.T, X[i])\n",
    "            sigma += max(0, 1-y[i] * margin)\n",
    "\n",
    "        l2 = (self.lamb/2) * (np.linalg.norm(self.omega)**2)\n",
    "\n",
    "        total_loss = l2 + sigma\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    \n",
    "    \n",
    "    # Gradient descent\n",
    "    def gradient(self, X, y):\n",
    "        sigma = 0\n",
    "        for i in range(len(X)):\n",
    "            if y[i] * (np.dot(self.omega.T, X[i])) >= 1:\n",
    "                sigma += 0\n",
    "            else:\n",
    "                sigma += -y[i]*X[i]\n",
    "\n",
    "        grad = (self.lamb*self.omega) + sigma\n",
    "        \n",
    "        return grad\n",
    "\n",
    "    def gradient_descent(self, X, y):\n",
    "        self.omega = self.omega - (self.gamma * self.gradient(X, y))\n",
    "\n",
    "        return self.omega\n",
    "\n",
    "    \n",
    "    \n",
    "    # Optimize best omega\n",
    "    def fit(self, X, y):\n",
    "        counter = 0\n",
    "        while self.loss(X, y) > 100 and counter < 1000:            \n",
    "            self.omega = self.gradient_descent(X, y)\n",
    "            counter+=1          # Counter to stop the loop when searching for parameters    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(self.omega.T, X))    \n",
    "  \n",
    "\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_hat = []\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            y_hat.append(self.predict(X[i]))\n",
    "        correct = 0\n",
    "        for i, number in enumerate(y_hat):\n",
    "            if y_hat[i] == y[i]:\n",
    "                correct += 1\n",
    "        accuracy = (correct/len(y_hat))*100\n",
    "        print(f'{accuracy}% correct')\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSlwPrX1wVCq"
   },
   "source": [
    "## Part 4: Exploring hyperparameters\n",
    " \n",
    "Much lower range of grid points than those originally suggested, since I knew from generating the model that larger hyperparameters would not work given my implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_points = np.exp(np.linspace(np.log(0.000001), np.log(.3), 10))\n",
    "\n",
    "for i, gamma in enumerate(grid_points[-1]):\n",
    "    for j, lamb in enumerate(grid_points[-1]):\n",
    "        model = Model(learning_rate=gamma, dampening=lamb)\n",
    "        model.fit(X, y)\n",
    "        grid[i, j] = model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid where the columns are the learning rate, and the rows are the dampening**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 94.8 ,  94.8 ,  94.8 ,  94.8 ,  94.8 ,  94.8 ,  94.8 ,  94.8 ,\n",
       "         94.8 ,  94.75],\n",
       "       [ 99.2 ,  99.2 ,  99.2 ,  99.2 ,  99.2 ,  99.2 ,  99.25,  99.25,\n",
       "         99.2 ,  99.3 ],\n",
       "       [ 98.5 ,  98.5 ,  98.5 ,  98.5 ,  98.7 ,  98.7 ,  98.7 ,  98.9 ,\n",
       "         98.45,  98.8 ],\n",
       "       [ 98.55,  98.55,  98.55,  98.55,  98.55,  98.55,  98.55,  98.5 ,\n",
       "         98.8 ,  98.9 ],\n",
       "       [ 99.15,  99.15,  99.15,  99.15,  99.15,  99.2 ,  99.2 ,  99.3 ,\n",
       "         99.4 , 100.  ],\n",
       "       [ 99.65,  99.65,  99.65,  99.65,  99.65,  99.75,  99.65,  99.9 ,\n",
       "        100.  , 100.  ],\n",
       "       [ 99.9 ,  99.9 ,  99.9 ,  99.9 ,  99.9 ,  99.95, 100.  , 100.  ,\n",
       "        100.  , 100.  ],\n",
       "       [ 99.95,  99.95,  99.95, 100.  , 100.  , 100.  , 100.  , 100.  ,\n",
       "        100.  ,  94.65],\n",
       "       [ 99.95,  99.95, 100.  , 100.  , 100.  , 100.  , 100.  , 100.  ,\n",
       "         52.25,  50.  ],\n",
       "       [100.  , 100.  , 100.  , 100.  , 100.  , 100.  , 100.  ,  53.75,\n",
       "         50.  ,  50.  ]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3, 0.00027181668598497995)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiple hyperparameter sets were 100% accurate on the training set, so I picked one out of those\n",
    "best_learning_rate, best_dampening = float(grid_points[9]), float(grid_points[4])\n",
    "best_learning_rate, best_dampening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsxPiVN2NG8M"
   },
   "source": [
    "## Part 5: Final test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(dampening=best_dampening, learning_rate=best_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_training, y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.5% correct\n"
     ]
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1 - Sentiment Polarity for Movie Reviews v2021.1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
