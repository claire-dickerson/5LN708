{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Wv4dtRHc71Q"
   },
   "source": [
    "# Assignment 3: Evaluating and extending an RNN based POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qDDstzhwztQS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGrLTi4Nc1yY",
    "outputId": "ca909c6e-3bfd-42e6-91a0-61672609e553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name(i))\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FisGZ4-PCMok"
   },
   "source": [
    "## Download and parse UD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9RRu7_gyK1J",
    "outputId": "7d7a6f1f-41c7-4456-f9bd-925c02a0d1d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  410M  100  410M    0     0  25.2M      0  0:00:16  0:00:16 --:--:-- 25.1M\n",
      "100 89.9M  100 89.9M    0     0  27.5M      0  0:00:03  0:00:03 --:--:-- 30.2M\n",
      "100  533k  100  533k    0     0  2051k      0 --:--:-- --:--:-- --:--:-- 2051k\n"
     ]
    }
   ],
   "source": [
    "!curl --remote-name-all https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3687{/ud-treebanks-v2.8.tgz,/ud-documentation-v2.8.tgz,/ud-tools-v2.8.tgz}\n",
    "!tar -xf ud-treebanks-v2.8.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "axvj9Po5zvZt"
   },
   "outputs": [],
   "source": [
    "def parse(file):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(file, 'r') as infile:\n",
    "        sents = infile.read().split('\\n\\n')\n",
    "        if sents[-1] == '':\n",
    "            sents = sents[:-1]\n",
    "        for sent in sents:\n",
    "            words, tags = [], []\n",
    "            lines = sent.split('\\n')\n",
    "            for line in lines:\n",
    "                if line.startswith('#'):\n",
    "                    continue\n",
    "                line = line.strip().split('\\t')\n",
    "                words.append(line[1])\n",
    "                tags.append(line[3])\n",
    "            X.append(words)\n",
    "            y.append(tags)\n",
    "            \n",
    "    assert len(X) == len(y)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMHQjqmaCH5r"
   },
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tszlm8cB4Plt"
   },
   "source": [
    "English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2gdLSR1grXFk"
   },
   "outputs": [],
   "source": [
    "treebank_train = '/content/ud-treebanks-v2.8/UD_English-EWT/en_ewt-ud-train.conllu'\n",
    "treebank_test = '/content/ud-treebanks-v2.8/UD_English-EWT/en_ewt-ud-train.conllu'\n",
    "X_train_en, y_train_en = parse(treebank_train)\n",
    "X_test_en, y_test_en = parse(treebank_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIKUMeDD4W0t"
   },
   "source": [
    "Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kVC0UNZt4V3u"
   },
   "outputs": [],
   "source": [
    "treebank_train = '/content/ud-treebanks-v2.8/UD_Swedish-LinES/sv_lines-ud-train.conllu'\n",
    "treebank_test = '/content/ud-treebanks-v2.8/UD_Swedish-LinES/sv_lines-ud-test.conllu'\n",
    "X_train_sv, y_train_sv = parse(treebank_train)\n",
    "X_test_sv, y_test_sv = parse(treebank_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9jl5SUK4YZh"
   },
   "source": [
    "Norwegian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "AoG6uOv54UVm"
   },
   "outputs": [],
   "source": [
    "treebank_train = '/content/ud-treebanks-v2.8/UD_Norwegian-Bokmaal/no_bokmaal-ud-train.conllu'\n",
    "treebank_test = '/content/ud-treebanks-v2.8/UD_Norwegian-Bokmaal/no_bokmaal-ud-test.conllu'\n",
    "X_train_no, y_train_no = parse(treebank_train)\n",
    "X_test_no, y_test_no = parse(treebank_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjc7isYz4bUt"
   },
   "source": [
    "Danish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "k1s2Y8_Z4Lgd"
   },
   "outputs": [],
   "source": [
    "treebank_train = '/content/ud-treebanks-v2.8/UD_Danish-DDT/da_ddt-ud-train.conllu'\n",
    "treebank_test = '/content/ud-treebanks-v2.8/UD_Danish-DDT/da_ddt-ud-test.conllu'\n",
    "X_train_da, y_train_da = parse(treebank_train)\n",
    "X_test_da, y_test_da = parse(treebank_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CengrMq4eSq"
   },
   "source": [
    "Icelandic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "gRfvWAtn4dsQ"
   },
   "outputs": [],
   "source": [
    "treebank_train = '/content/ud-treebanks-v2.8/UD_Icelandic-Modern/is_modern-ud-train.conllu'\n",
    "treebank_test = '/content/ud-treebanks-v2.8/UD_Icelandic-Modern/is_modern-ud-test.conllu'\n",
    "X_train_is, y_train_is = parse(treebank_train)\n",
    "X_test_is, y_test_is = parse(treebank_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjj6HTAf4RLc"
   },
   "source": [
    "Faroese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "L_RziBcz_hJ5"
   },
   "outputs": [],
   "source": [
    "treebank_train = '/content/ud-treebanks-v2.8/UD_Faroese-FarPaHC/fo_farpahc-ud-train.conllu'\n",
    "treebank_test = '/content/ud-treebanks-v2.8/UD_Faroese-FarPaHC/fo_farpahc-ud-test.conllu'\n",
    "X_train_fo, y_train_fo = parse(treebank_train)\n",
    "X_test_fo, y_test_fo = parse(treebank_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHIiYxLyz2cf"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fjVkyDOP9MU",
    "outputId": "59aa2abd-2cd0-437d-d5e5-f544f1590f7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 910,   63, 2063,  901, 1551, 2046,  901, 2063, 1790, 1477, 1551, 2046,\n",
       "           901, 2063, 1339, 1842, 2297, 2297, 2297, 2297, 2297, 2297],\n",
       "         [2155, 2063,  754,   63, 1790, 1477, 1842, 2297, 2297, 2297, 2297, 2297,\n",
       "          2297, 2297, 2297, 2297, 2297, 2297, 2297, 2297, 2297, 2297],\n",
       "         [2239, 1412,  103, 1318,  989,  635, 1186, 1551, 2046, 1296, 1926, 2270,\n",
       "          1575,  989,  730, 1186, 1551, 1415,  989,   91, 1899, 1842],\n",
       "         [ 910, 1186, 2063, 1375, 1551, 2046,  966, 2063,  136, 2277, 1842, 2297,\n",
       "          2297, 2297, 2297, 2297, 2297, 2297, 2297, 2297, 2297, 2297],\n",
       "         [ 342, 2174, 1302,  754, 1146, 1551, 2046, 1915, 2208, 1489,  635, 1186,\n",
       "          1842, 2297, 2297, 2297, 2297, 2297, 2297, 2297, 2297, 2297]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 6,  0,  9,  3,  5,  8,  3,  9,  6, 14,  5,  8,  3,  9, 14,  5, 17, 17,\n",
       "          17, 17, 17, 17],\n",
       "         [ 7,  9,  6,  0,  6, 14,  5, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
       "          17, 17, 17, 17],\n",
       "         [ 2,  3,  9, 16,  6,  6,  7,  5,  8,  6,  7, 16,  2,  6,  6,  7,  5, 15,\n",
       "           6,  9, 16,  5],\n",
       "         [ 6,  7,  9,  3,  5,  8,  3,  9,  3,  3,  5, 17, 17, 17, 17, 17, 17, 17,\n",
       "          17, 17, 17, 17],\n",
       "         [ 8,  3, 16,  6,  3,  5,  8,  3, 16,  4,  6,  7,  5, 17, 17, 17, 17, 17,\n",
       "          17, 17, 17, 17]], device='cuda:0'))"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = {token for sentence in X_train_fo for token in sentence}\n",
    "idx2token = list(tokens)\n",
    "idx2token.insert(0, '<UNK>')\n",
    "idx2token.append('<PAD>')\n",
    "token2idx = {token:idx for idx, token in enumerate(idx2token)}\n",
    "\n",
    "tags = {tag for tags in y_train_fo for tag in tags}\n",
    "idx2tag = list(tags)\n",
    "idx2tag.append('<PAD>')\n",
    "tag2idx = {tag:idx for idx, tag in enumerate(idx2tag)}\n",
    "\n",
    "\n",
    "def pad_and_encode(sentences, labels):\n",
    "    assert len(sentences)==len(labels)\n",
    "    assert np.all([len(sentence)==len(tags) for sentence, tags in zip(sentences, labels)])\n",
    "    max_sentence_length = np.max([len(sentence) for sentence in sentences])\n",
    "    padded_sentences = torch.zeros(len(sentences), max_sentence_length,    \n",
    "                                    dtype=torch.long)\n",
    "    padded_sentences[:] = token2idx['<PAD>']\n",
    "    padded_labels = torch.zeros(len(sentences), max_sentence_length, \n",
    "                                dtype=torch.long)\n",
    "    padded_labels[:] = tag2idx['<PAD>']\n",
    "    for i, (sentence, tags) in enumerate(zip(sentences, labels)):               \n",
    "        for j, token in enumerate(sentence):\n",
    "            if token in token2idx.keys():\n",
    "                padded_sentences[i, j] = token2idx[token]\n",
    "            else:\n",
    "                padded_sentences[i, j] = token2idx['<UNK>']\n",
    "        for j, tag in enumerate(tags):\n",
    "            padded_labels[i, j] = tag2idx[tag]\n",
    "    return padded_sentences, padded_labels\n",
    "\n",
    "\n",
    "def batch_iterator(sentences, labels, batch_size=64):\n",
    "    \"\"\"Helper function for iterating over batches of the data\"\"\"\n",
    "    assert len(sentences) == len(labels)\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        X, y = pad_and_encode(sentences[i:min(i+batch_size, len(sentences))], \n",
    "                            labels[i:min(i+batch_size, len(sentences))])\n",
    "        if torch.cuda.is_available():                                               \n",
    "            yield (X.cuda(), y.cuda())\n",
    "        else:\n",
    "            yield (X, y)\n",
    "\n",
    "next(batch_iterator(X_train_fo, y_train_fo, batch_size=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmgbWbsJk8DW"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "BznpqMpWcQkG"
   },
   "outputs": [],
   "source": [
    "class Tagger(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, model_hidden_dim, vocabulary_size, tagset_size, model='lstm', num_layers=1, dropout=.5):\n",
    "        super(Tagger, self).__init__()                                          \n",
    "        self.model_hidden_dim_ = model_hidden_dim                                     \n",
    "        self.vocabulary_size_ = vocabulary_size\n",
    "        self.tagset_size_ = tagset_size\n",
    "        self.num_layers_ = num_layers                                           # allow for stacked model\n",
    "        self.dropout_ = dropout                                                 # introduce dropout, default = 50%\n",
    "        self.model = model                                                      # allow for GRU option, default LSTM\n",
    "\n",
    "        self._word_embedding = nn.Embedding(num_embeddings=vocabulary_size,         \n",
    "                                            embedding_dim=word_embedding_dim, \n",
    "                                            padding_idx=token2idx['<PAD>'])\n",
    "        if self.model == 'lstm': \n",
    "            self._lstm = nn.LSTM(input_size=word_embedding_dim,                        \n",
    "                                hidden_size=model_hidden_dim,                           \n",
    "                                num_layers=num_layers,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout)\n",
    "        elif self.model == 'gru':\n",
    "            self._lstm = nn.GRU(input_size=word_embedding_dim,                         \n",
    "                                hidden_size=model_hidden_dim,                          \n",
    "                                batch_first=True,\n",
    "                                num_layers=num_layers,\n",
    "                                dropout=dropout)\n",
    "        self._fc = nn.Linear(model_hidden_dim, tagset_size)                         \n",
    "        self._softmax = nn.LogSoftmax(dim=1)                                        \n",
    "\n",
    "        if torch.cuda.is_available():                                               \n",
    "            self.cuda()\n",
    "\n",
    "\n",
    "    def forward(self, padded_sentences):\n",
    "        \"\"\"The forward pass through the network\"\"\"\n",
    "        batch_size, max_sentence_length = padded_sentences.size()\n",
    "\n",
    "        embedded_sentences = self._word_embedding(padded_sentences)   \n",
    "\n",
    "        sentence_lengths = (padded_sentences!=token2idx['<PAD>']).sum(dim=1)\n",
    "        sentence_lengths = sentence_lengths.long().cpu()\n",
    "        X = nn.utils.rnn.pack_padded_sequence(embedded_sentences, sentence_lengths, \n",
    "                                            batch_first=True, enforce_sorted=False)\n",
    "        lstm_out, _ = self._lstm(X)                                               \n",
    "        X, _ = nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=True)        \n",
    "\n",
    "        X = X.contiguous().view(-1, X.shape[2])                                 \n",
    "        tag_space = self._fc(X)                 \n",
    "        tag_scores = self._softmax(tag_space)                       \n",
    "        return tag_scores.view(batch_size, max_sentence_length, self.tagset_size_)\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"Training the network\"\"\"\n",
    "        loss_function = nn.NLLLoss(ignore_index=tag2idx['<PAD>'])\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01) \n",
    "\n",
    "        batch_size = 256  \n",
    "        for epoch in range(5):  \n",
    "            with tqdm(batch_iterator(X_train, y_train, batch_size=batch_size), \n",
    "                    total=len(X_train)//batch_size+1, unit=\"batch\", desc=\"Epoch %i\" % epoch) as batches:\n",
    "                for inputs, targets in batches:   \n",
    "                    self.zero_grad()            \n",
    "                    scores = self(inputs)      \n",
    "                    loss = loss_function(scores.view(-1, self.tagset_size_),   \n",
    "                                        targets.view(-1))               \n",
    "                    loss.backward()\n",
    "                    optimizer.step()   \n",
    "                    predictions = scores.argmax(dim=2, keepdim=True).squeeze() \n",
    "                    mask = targets!=tag2idx['<PAD>']  \n",
    "                    correct = (predictions[mask] == targets[mask]).sum().item()  \n",
    "                    accuracy = correct / mask.sum().item()*100\n",
    "                    batches.set_postfix(loss=loss.item(), accuracy=accuracy)\n",
    "\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        \"\"\"Get the accuracy of the model given the data\"\"\"\n",
    "        with torch.no_grad():\n",
    "            n_correct = 0\n",
    "            n_total = 0\n",
    "            for inputs, targets in batch_iterator(X_test, y_test, batch_size=64): \n",
    "                scores = self(inputs)\n",
    "                predictions = scores.argmax(dim=2, keepdim=True).squeeze()\n",
    "                mask = targets!=tag2idx['<PAD>'] \n",
    "                n_correct += (predictions[mask] == targets[mask]).sum().item() \n",
    "                n_total += mask.sum().item()\n",
    "        print(\"Test accuracy %.1f%%\" % (100*n_correct/n_total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yr38d_mEhCLj",
    "outputId": "e677eee8-8d46-4461-d060-36dc8818f5a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = Tagger(word_embedding_dim=64,\n",
    "               model_hidden_dim=128,\n",
    "               vocabulary_size=len(token2idx),\n",
    "               tagset_size=len(tag2idx)-1,\n",
    "               model='gru')                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khAr2Ykt7m4y"
   },
   "source": [
    "## Quantitative results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4xvF9Lr-Gqr"
   },
   "source": [
    "English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muMMKawahjLK",
    "outputId": "7fa92d82-eef6-4a1c-9cd3-e2bed0b53afb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 49/49 [00:02<00:00, 19.12batch/s, accuracy=94.6, loss=0.158]\n",
      "Epoch 1: 100%|██████████| 49/49 [00:02<00:00, 19.33batch/s, accuracy=96.2, loss=0.111]\n",
      "Epoch 2: 100%|██████████| 49/49 [00:02<00:00, 19.10batch/s, accuracy=96.7, loss=0.0961]\n",
      "Epoch 3: 100%|██████████| 49/49 [00:02<00:00, 19.16batch/s, accuracy=97.6, loss=0.0696]\n",
      "Epoch 4: 100%|██████████| 49/49 [00:02<00:00, 19.44batch/s, accuracy=98.2, loss=0.0568]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_en, y_train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6OMC5bl8sox",
    "outputId": "060a5d5a-ad15-492c-da85-fbbcc2a13823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 98.6%\n"
     ]
    }
   ],
   "source": [
    "model.score(X_test_en, y_test_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElTHDQfc-IP1"
   },
   "source": [
    "Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XM-rS7K8hmWE",
    "outputId": "263581a4-a21a-475d-a645-6d00f84086f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 13/13 [00:00<00:00, 19.40batch/s, accuracy=63, loss=1.17]\n",
      "Epoch 1: 100%|██████████| 13/13 [00:00<00:00, 19.51batch/s, accuracy=74.4, loss=0.733]\n",
      "Epoch 2: 100%|██████████| 13/13 [00:00<00:00, 19.66batch/s, accuracy=83.9, loss=0.494]\n",
      "Epoch 3: 100%|██████████| 13/13 [00:00<00:00, 19.62batch/s, accuracy=89.7, loss=0.34]\n",
      "Epoch 4: 100%|██████████| 13/13 [00:00<00:00, 19.65batch/s, accuracy=93.3, loss=0.226]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_sv, y_train_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWo8csVY-dkp",
    "outputId": "5eb2338a-2159-4e27-dcfb-e287d23ff235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 78.9%\n"
     ]
    }
   ],
   "source": [
    "model.score(X_test_sv, y_test_sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49cxI6ZlChpv"
   },
   "source": [
    "Norwegian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixENAVZ62_9q",
    "outputId": "7e7a31bd-4c41-4020-a8f0-1651be24a5af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 62/62 [00:02<00:00, 21.24batch/s, accuracy=76.8, loss=0.661]\n",
      "Epoch 1: 100%|██████████| 62/62 [00:02<00:00, 21.41batch/s, accuracy=87.5, loss=0.353]\n",
      "Epoch 2: 100%|██████████| 62/62 [00:02<00:00, 21.60batch/s, accuracy=94.9, loss=0.168]\n",
      "Epoch 3: 100%|██████████| 62/62 [00:02<00:00, 21.34batch/s, accuracy=97.3, loss=0.0894]\n",
      "Epoch 4: 100%|██████████| 62/62 [00:02<00:00, 21.49batch/s, accuracy=98.7, loss=0.0547]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_no, y_train_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYf9AjGE7lIP",
    "outputId": "980f0a17-6390-40a7-c3c9-50e20c51b309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 86.5%\n"
     ]
    }
   ],
   "source": [
    "model.score(X_test_no, y_test_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eq9IYmD6Ckw7"
   },
   "source": [
    "Danish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MX9qGZuWCs7G",
    "outputId": "844956f1-3a9d-4ffb-e07e-1e25a6bb2235"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 18/18 [00:00<00:00, 18.70batch/s, accuracy=68.4, loss=1]\n",
      "Epoch 1: 100%|██████████| 18/18 [00:00<00:00, 18.40batch/s, accuracy=82.1, loss=0.555]\n",
      "Epoch 2: 100%|██████████| 18/18 [00:00<00:00, 18.61batch/s, accuracy=92.8, loss=0.291]\n",
      "Epoch 3: 100%|██████████| 18/18 [00:00<00:00, 18.69batch/s, accuracy=97.8, loss=0.136]\n",
      "Epoch 4: 100%|██████████| 18/18 [00:00<00:00, 18.76batch/s, accuracy=98.9, loss=0.0696]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_da, y_train_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x6QYJw_lCwvQ",
    "outputId": "0a97facc-33ee-4abc-d68b-c09bd476a918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 81.4%\n"
     ]
    }
   ],
   "source": [
    "model.score(X_test_da, y_test_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ3pfoxJC0jA"
   },
   "source": [
    "Icelandic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDVcpWjRDAIQ",
    "outputId": "089a4db3-5410-49bd-c5f3-35d40b9f82dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  95%|█████████▌| 21/22 [00:01<00:00, 14.33batch/s, accuracy=60.6, loss=1.27]\n",
      "Epoch 1:  95%|█████████▌| 21/22 [00:01<00:00, 14.30batch/s, accuracy=76.1, loss=0.73]\n",
      "Epoch 2:  95%|█████████▌| 21/22 [00:01<00:00, 14.25batch/s, accuracy=88.1, loss=0.386]\n",
      "Epoch 3:  95%|█████████▌| 21/22 [00:01<00:00, 14.52batch/s, accuracy=95.1, loss=0.192]\n",
      "Epoch 4:  95%|█████████▌| 21/22 [00:01<00:00, 14.63batch/s, accuracy=97.8, loss=0.0981]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_is, y_train_is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UaBd6PjdDEL4",
    "outputId": "c5b3170e-bbd2-49b8-de5f-e46bb6dea26e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 94.4%\n"
     ]
    }
   ],
   "source": [
    "model.score(X_test_is, y_test_is)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoLT8Hd3DJZN"
   },
   "source": [
    "Faroese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSBQQ4XtDIxQ",
    "outputId": "18b237bc-a2b4-4dbd-c908-5b88d45f0b1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 4/4 [00:00<00:00, 15.13batch/s, accuracy=37.7, loss=1.97]\n",
      "Epoch 1: 100%|██████████| 4/4 [00:00<00:00, 15.34batch/s, accuracy=65.5, loss=1.25]\n",
      "Epoch 2: 100%|██████████| 4/4 [00:00<00:00, 13.27batch/s, accuracy=72.6, loss=0.889]\n",
      "Epoch 3: 100%|██████████| 4/4 [00:00<00:00, 14.71batch/s, accuracy=80.2, loss=0.678]\n",
      "Epoch 4: 100%|██████████| 4/4 [00:00<00:00, 14.58batch/s, accuracy=84.3, loss=0.523]\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_fo, y_train_fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h3_C3e5RDcQA",
    "outputId": "1c42d194-4454-49df-989f-678ec159af50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 71.0%\n"
     ]
    }
   ],
   "source": [
    "model.score(X_test_fo, y_test_fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yG-HqC8v3Asi"
   },
   "source": [
    "### Baseline\n",
    "\n",
    "*for English*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xe3nWD4chrkH",
    "outputId": "05f845c2-2ce2-41d4-96c4-3f577ec36abf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16791973902636761"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "baseline_X = []\n",
    "for sentence in X_train_en:\n",
    "    baseline_X.extend(sentence)\n",
    "\n",
    "baseline_y = []\n",
    "for sentence in y_train_en:\n",
    "    baseline_y.extend(sentence)\n",
    "\n",
    "baseline = DummyClassifier(strategy='most_frequent')\n",
    "baseline.fit(baseline_X, baseline_y)\n",
    "\n",
    "baseline_X_test = []\n",
    "for sentence in X_test_en:\n",
    "    baseline_X_test.extend(sentence)\n",
    "\n",
    "baseline_y_test = []\n",
    "for sentence in y_test_en:\n",
    "    baseline_y_test.extend(sentence)\n",
    "\n",
    "baseline.score(baseline_X_test, baseline_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FhzE03N5ox3"
   },
   "source": [
    "## Report\n",
    "\n",
    "As extensions, I chose to implement a GRU option, dropout, and comparing my tagger to the current state-of-the-art. Perhaps unsurprisingly, the model performs consistently more poorly on all data than the state-of-the-art. For English, I was able to get an accuracy of about 97% on the EWT data set, while [state-of-the-art](https://arxiv.org/abs/1906.01569) is at approximately 96%. In terms of methodology, the biggest difference between my model and the state-of-the-art is that they tend to use much larger embeddings like BERT. They also use [bi-LSTMs](https://arxiv.org/abs/1604.05529), which I could have implemented but did not. This is probably my model's largest shortcoming, since a bi-LSTM would likely increase robustness. Additionally, extending my model to the character level would probably help too, but at the end of the day my model still does quite well, which is somewhat surprising when comparing to the state-of-the-art. However, it's also not so surprising at the end of the day since English is such a high resource language with a good amount of training data available.\n",
    "\n",
    "In order to get this decent accuracy, I had to increase the dimensionality of my embeddings quite a bit. This does support the trend that a more complex model leads to better accuracy, but I do not want to say that is a rule. As with all models, at a certain point increasing complexity does not contribute significantly to better accuracy, and the more important factor is how much data is available to train on. Even looking at the best performing models on NLP progress, the average accuracy is within 1 percentage point. This is, however, using clean, UD data. Looking at the social media data, the scores are quite a bit lower since the data is messy, and no amount of complexity can (probably) fix that. At least at this point.\n",
    "\n",
    "However, on some of the Scandinavian languages I tested, like Faroese with a much smaller data set of only 40,000 tokens, the model performs quite a bit worse. This indicates that the size of the data set has a greater impact. I did, though, find that a slight change in dimensionality had a much greater impact on this small data set. By increasing the dimensionalities from 62 to 128 and 128 to 256, the model acuracy jumped by about 10 percentage points.\n",
    "\n",
    "Across all languages, though, my model is consistently better than a majority baseline that simply guesses the most common tag for each word. For English, this was a little under 20%, so my 97% is much better. \n",
    "\n",
    "Overall, implementing extensions was not too difficult, as it generally required simply adding one or two lines of code since the PyTorch API is quite consistent. However, I did find it significantly harder to implement the model as an sk-learn class than it needed to be. After speaking with some of the TAs, I decided to do pre-processing before implementing the class, which made life a lot simpler, and in reality is still consistent with sk-learn classifiers which require pre-processed data. Additionally, I decided not to implement a ```predict``` method since it was not a part of the code we were given, and it was not necessary for the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nj4J_avAEx4F"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
